{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5dd0cd",
   "metadata": {},
   "source": [
    "# Amazon Bedrock - Zero to Hero - Prepared for Twilio Hackathon (Tweek Week)\n",
    "\n",
    "### *You have an Idea >> want generation/search workflow backed by an LLM >> Want minimal infra setup >> Use Bedrock*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e8a84",
   "metadata": {},
   "source": [
    "# # Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common tasks\n",
    "# Install required packages. Ensure, Python version > 3.10\n",
    "!pip -q install -r requirements.txt\n",
    "\n",
    "# Import python SDK\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "## Initialize Python client object for bedrock-runtime service\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', region_name='us-west-2')\n",
    "\n",
    "model_id = \"amazon.titan-text-lite-v1\"  # Replace with your model ID\n",
    "prompt = \"What is the meaning of love?\"  # Replace with your prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ccf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My first Bedrock Invoke API Call\n",
    "response = bedrock.invoke_model(\n",
    "    body=f'{{\"inputText\": \"{prompt}\"}}',\n",
    "    modelId=model_id)\n",
    "\n",
    "print(response)  # Print the raw response for debugging\n",
    "\n",
    "result= response['body'].read().decode('utf-8') # Decode the response body as it is serialized in bytes\n",
    "print(json.dumps(result,indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dafe81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ## Bedrock Features and Capabilities\n",
    "\n",
    "- **Fully Managed**: No need to manage infrastructure; AWS handles scaling, security, and maintenance.\n",
    "- **Foundation Models**: Access to a variety of pre-trained models from AI startups and Amazon.\n",
    "- **Knowledge Bases**: Create and manage knowledge bases to enhance the capabilities of your applications.\n",
    "- **Guardrails**: Built-in safety features to ensure responsible AI usage, including content filtering and moderation.\n",
    "- **Customization**: Fine-tune models with your own data to improve performance for specific tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d573c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[Bedrock Capability]] Hot-swapping model IDs for testing\n",
    "# model_id1 = \"amazon.titan-text-premier-v1:0\" # for us-east-1\n",
    "model_id1 = \"amazon.titan-tg1-large\"            # for us-west-2\n",
    "\n",
    "# Make native request suitable for Titan Text family models.\n",
    "# Format the request payload using the model's native structure.\n",
    "native_request = {\n",
    "    \"inputText\": prompt,\n",
    "    \"textGenerationConfig\": {\n",
    "        \"maxTokenCount\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)\n",
    "\n",
    "# Invoke the model using the Bedrock client.\n",
    "response = bedrock.invoke_model(\n",
    "    modelId=model_id1,\n",
    "    contentType='application/json',\n",
    "    accept='application/json',\n",
    "    body=request\n",
    ")\n",
    "\n",
    "# Decode the response body as it is serialized in bytes\n",
    "# result = response[\"body\"].read().decode('utf-8')\n",
    "# print(json.dumps(result, indent=2))\n",
    "\n",
    "# Decode the response body.\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "print (model_response)\n",
    "# Extract and print the response text.\n",
    "response_text = model_response[\"results\"][0][\"outputText\"]\n",
    "print(response_text)\n",
    "\n",
    "## model_id2 = \"anthropic.claude-3-5-sonnet-20240620-v1:0\" \n",
    "## DISCUSS: The anthropic model won't work as is for below function, as it requires a different body structure.\n",
    "## We solve for this later. \n",
    "## Typically, hot-swapping models happen for testing different versions of same model class. Bedrock also supports fine-tuning of models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Bedrock Client for Admin \n",
    "bedrock_client = boto3.client(service_name='bedrock', region_name='us-west-2')\n",
    "\n",
    "# List Foundation Models\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html#model-ids-arns\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html\n",
    "\n",
    "response = bedrock_client.list_foundation_models()\n",
    "print(json.dumps(response, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581b254",
   "metadata": {},
   "source": [
    "# # Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beaefb7",
   "metadata": {},
   "source": [
    "## ## Let's look at customizations and more client libraries for Bedrock (under python/boto3)\n",
    "\n",
    "1. [Bedrock](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock.html)\n",
    "2. [Bedrock-runtime](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime.html)\n",
    "3. [bedrock-agent](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent.html)\n",
    "4. [bedrock-agent-runtime](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime.html)\n",
    "\n",
    "Other API calls:\n",
    "\n",
    "```python\n",
    "bedrock.list_foundation_models | create_guardrail | create_custom_model | create_custom_model_deployment | create_evaluation_job\n",
    "bedrock-runtime.invoke_model | invoke_model_with_response_stream | converse | converse_with_response_stream\n",
    "bedrock-agent.create_agent | create_knowledge_base | associate_agent_knowledge_base | associate_agent_collaborators\n",
    "bedrock-agent-runtime.invoke_agent| invoke_flow | retreive | rerank | retreive_and_generate | retreive_and_generate_stream\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's Stream responses\n",
    "\n",
    "## Function to invoke_model_with_response_stream and print the output\n",
    "def invoke_model_with_response_stream(prompt, model_id):\n",
    "    response_stream = bedrock.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        body=f'{{\"inputText\": \"{prompt}\"}}'\n",
    "    )\n",
    "    \n",
    "    for event in response_stream['body']:\n",
    "        event = event['chunk']['bytes'].decode('utf-8') # Decode each event from bytes to string\n",
    "        event = json.loads(event)['outputText'] # load the JSON string into dictionary and get outputText\n",
    "        print(event, end='', flush=True)  # Print streaming output. Set end='' to avoid newlines and flush=True to ensure immediate output\n",
    "    print()  # Print a newline at the end  \n",
    "invoke_model_with_response_stream(prompt, model_id1)  # Call the function with the model ID and prompt\n",
    "\n",
    "## QUIZ: Why \"stream\"? if the response content is the same? \n",
    "## DISCUSS: DEEPSEEK'S   \"WAIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f54301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': {'role': 'assistant', 'content': [{'text': '\\n\\nThe model is unable to give an opinion on the meaning of love, but can provide general information on the topic. Love is a complex emotion that can be expressed in various ways, such as through kindness, compassion, and physical intimacy. It is a deeply personal and subjective experience that can vary greatly from person to person. \\r\\n\\r\\nLove can be found in many different forms, including romantic relationships, friendships, and familial bonds. At its core, love is about connecting with another person on a deep level and wanting the best for them. While the meaning of love can vary, it is a fundamental aspect of the human experience and is essential for human connection and happiness.'}]}}\n"
     ]
    }
   ],
   "source": [
    "# Use converse API to have a conversation with the model via Messages\n",
    "def converse_with_model(prompt, model_id):\n",
    "    \"\"\"\n",
    "    Provides a consistent interface that works with all models that support messages\n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        'role': 'user', \n",
    "        'content': [{'text': prompt}]\n",
    "        }]  # Create a message structure with role and content\n",
    "\n",
    "    response = bedrock.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages\n",
    "        )\n",
    "    return response['output']\n",
    "\n",
    "output = converse_with_model(prompt, model_id1)\n",
    "print(output)  # Print the output of the conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, lets customize the model with inferenceConfig\n",
    "\n",
    "\"\"\"When you make inference calls to models with the model invocation \n",
    "(InvokeModel, InvokeModelWithResponseStream, Converse, and ConverseStream) API operations, \n",
    "you include request parameters depending on the model that you're using.\n",
    "Ref: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\n",
    "\n",
    "Below is an example of how to use the inferenceConfig parameter in the InvokeModel API operation.\n",
    "\"\"\"\n",
    "titan_inference_config = {        # This works for Titan Text models.\n",
    "    \"maxTokenCount\": 10,    # Output tokens do not have a budget. Different keys, for different models. If too low, response can be unfinished with exception(e) thrown\n",
    "    \"temperature\": 0.7,     # Controls the randomness. Lower values make the output more deterministic. Rebalance probabilities to 1. Some models take >1. Change by large numbers. \n",
    "    \"topP\": 0.9             # Controls the diversity. Lower values make the output more focused. Of top-k (which is not exposed by all models), this reduces the top selection to given probability (p) threshold. Rebalance probabilities to 1. \n",
    "}                           # DISCUSS: How to use these parameters effectively? LONG YELLOW FRUIT example?\n",
    "\n",
    "\n",
    "custom_request = {\n",
    "    \"inputText\": prompt,\n",
    "    \"textGenerationConfig\": titan_inference_config  # Include the inference configuration in the request\n",
    "}\n",
    "custom_request = json.dumps(custom_request)\n",
    "\n",
    "# Invoke the model with the inference configuration\n",
    "response = bedrock.invoke_model(\n",
    "    modelId=model_id1,\n",
    "    contentType='application/json',\n",
    "    accept='application/json',\n",
    "    body=custom_request\n",
    ")\n",
    "result = response['body'].read().decode('utf-8')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd33f09",
   "metadata": {},
   "source": [
    "# # Walk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2184f6",
   "metadata": {},
   "source": [
    "## ## What else Amazon Bedrock can do besides LLM backed APIs (Aug 2025)?\n",
    "\n",
    "Now, that you can crawl with Bedrock, lets learn additional Bedrock Capabilities for Agentic AI\n",
    "\n",
    "1. Prompt Engineering\n",
    "2. Agents\n",
    "3. Knowledge Bases\n",
    "4. Guardrails \n",
    "5. Evaluations\n",
    "6. ...and more\n",
    "\n",
    "Next, we will look at Bedrock Console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb72a75",
   "metadata": {},
   "source": [
    "Below, I have already created (via AWS console) a guardrail. I am using the guardrail identifier below.\n",
    "\n",
    "Guardrails can be implemented over any invoke or converse API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d406b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets learn how to use guardrails for model invocation\n",
    "\n",
    "response = bedrock.invoke_model(\n",
    "    body = request,\n",
    "    modelId = model_id1,\n",
    "    guardrailIdentifier = \"3r1nkkv80wth\",\n",
    "    guardrailVersion =\"DRAFT\", \n",
    "    trace = \"ENABLED\"\n",
    ")\n",
    "\n",
    "# Decode the response body as it is serialized in bytes\n",
    "result = response[\"body\"].read().decode('utf-8')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5eb376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets use OpenAI models via Bedrock\n",
    "\n",
    "# Initialize the Bedrock Runtime client for 'us-west-2' region\n",
    "client = boto3.client(service_name='bedrock-runtime', region_name='us-west-2')\n",
    "\n",
    "# Model ID\n",
    "model_id = 'openai.gpt-oss-20b-1:0'\n",
    "\n",
    "# Inference configuration\n",
    "inference_config = {\n",
    "    \"maxTokens\": 512,        \n",
    "    \"temperature\": 0.7,      \n",
    "    \"topP\": 0.9              \n",
    "}\n",
    "\n",
    "# Create the request body\n",
    "native_request = {\n",
    "  \"model\": model_id, # You can omit this field\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\", \n",
    "      \"content\": \"Hello! How can I help you today?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt\n",
    "    }\n",
    "  ],\n",
    "  \"max_completion_tokens\": inference_config[\"maxTokens\"],   # OpenAI expects this field\n",
    "  \"temperature\": inference_config[\"temperature\"],           # OpenAI expects this field\n",
    "  \"top_p\": inference_config[\"topP\"],                        # OpenAI expects this field\n",
    "  \"stream\": False                                           # You can omit this field\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Make the InvokeModel request\n",
    "response = client.invoke_model(\n",
    "    modelId=model_id,\n",
    "    body=json.dumps(native_request)\n",
    "    )\n",
    "\n",
    "# Parse and print the message for each choice in the chat completion\n",
    "response_body = json.loads(response['body'].read().decode('utf-8'))\n",
    "\n",
    "print(response_body)  # Print the entire response body for debugging\n",
    "\n",
    "for choice in response_body['choices']:\n",
    "    print(choice['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of Bedrock-Agents boto3 client\n",
    "## Boto3 client docs: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock.html\n",
    "\n",
    "## This is almost an example of how \"not\" to use Bedrock Agents today, as it is a low-level API. \n",
    "## We will use Strands later to create agents with more advanced capabilities.\n",
    "## For now, we will use this to create and prepare an agent for use. \n",
    "\n",
    "import boto3\n",
    "agents_client = boto3.client('bedrock-agent', region_name='us-east-1')\n",
    "# Create an agent\n",
    "response = agents_client.create_agent(\n",
    "    agentName='MyAgent',\n",
    "    foundationModel='amazon.titan-text-premier-v1:0'\n",
    ")\n",
    "print(response)  # Print the response from creating the agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618936fc",
   "metadata": {},
   "source": [
    "# # Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5899853",
   "metadata": {},
   "source": [
    "## ## Strands\n",
    "\n",
    "### Strands Python SDK features Overview\n",
    "1. **Lightweight & Flexible**: Simple agent loop that just works and is fully customizable\n",
    "2. **Model Agnostic**: Support for Amazon Bedrock, Anthropic, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers\n",
    "3. **Advanced Capabilities**: Multi-agent systems, autonomous agents, and streaming support\n",
    "4. **Built-in MCP**: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools\n",
    "\n",
    "### Basic Concepts\n",
    "\n",
    "Strands Agents is a framework for building AI agents that can interact with AWS services and perform complex tasks. The key components are:\n",
    "\n",
    "1. **Agent**: The core component that manages the conversation and orchestrates tools\n",
    "2. **Model**: The underlying LLM (Large Language Model) that powers the agent\n",
    "3. **Tools**: Functions that the agent can use to perform specific tasks\n",
    "4. **Sessions** and State: Mechanisms for maintaining conversation history and agent state across interactions\n",
    "5. **Agent Loop**: The process flow of how agents receive input, process it, and generate responses\n",
    "6. **Context Management**: How agents maintain and manage conversation context, including memory and retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafb593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Strands Python SDK (requires Python 3.10+)\n",
    "!pip install strands-agents strands-agents-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to build an agent using Strands\n",
    "from strands import Agent\n",
    "\n",
    "agent = Agent()\n",
    "response = agent(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize Strands' Defaults\n",
    "\n",
    "# The default model provider is Amazon Bedrock and the default model is Claude 4 Sonnet in the US Oregon (us-west-2) region.\n",
    "\n",
    "agent = Agent(model=\"amazon.nova-pro-v1:0\")\n",
    "response = agent(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand Model Providers\n",
    "\n",
    "# Strands is model agnostic and provides a unified interface to work with different model providers \n",
    "# using a `Model` class available via the `strands.models` module.\n",
    "# Here, I am using BedrockModel to configu\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7,  # Adjust temperature for response randomness\n",
    "    top_p=0.9,  # Adjust top_p for response diversity\n",
    ")\n",
    "\n",
    "agent = Agent(model=bedrock_model)\n",
    "response = agent(prompt)\n",
    "# print(response)\n",
    "# Always wrap your calls in Try/Except clauses. \n",
    "# DISCUSS: WHY? Tokens are not free (atleast today), and reasons to control verbosity. Try changing max_tokens to 10, and see the difference.\n",
    "# DISCUSS: Token Budgets. And how model implementations differ in handling them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b758b33",
   "metadata": {},
   "source": [
    "Lets understand tools.\n",
    "\n",
    "Tools can be any of three types:\n",
    "1. Python function call, decorated as a @tool \n",
    "2. Built-in tools, imported using strands_tools\n",
    "3. Path to a python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac550a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TOOLS to the Agent\n",
    "from strands import Agent, tool\n",
    "\n",
    "# Import the calculator tool.\n",
    "from strands_tools import calculator # Import the calculator tool\n",
    "# https://github.com/strands-agents/tools --> List of built-in tools. \n",
    "\n",
    "# Create a custom tool \n",
    "@tool\n",
    "def weather():\n",
    "    \"\"\" Get weather \"\"\" # Dummy implementation\n",
    "    return \"Always sunny in Philly\"\n",
    "\n",
    "agent = Agent(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",  # Optional: Specify the model ID\n",
    "    tools=[calculator, weather],\n",
    "    system_prompt=\"You are a helpful assistant who can only talk in rhymes. You can do simple math calculation and tell the weather.\")\n",
    "\n",
    "print (agent(\"What is 2+2?\"))  # Use the calculator tool\n",
    "\n",
    "response = agent(\"What is the weather today?\")\n",
    "# print(response)\n",
    "# Sharp eyes can tell, you do NOT need to print. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12339ee",
   "metadata": {},
   "source": [
    "Strands is model-agnostic. Lets use OpenAI direct API calls using Strands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445230ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an Agent with OpenAI directly - using Strands\n",
    "%pip install 'strands-agents[openai]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OpenAI model using Strands (I do not have an API Key)\n",
    "from strands import Agent\n",
    "from strands.models.openai import OpenAIModel\n",
    "import os\n",
    "api = os.environ.get(\"OpenAI_API_KEY\", \"Really??\")\n",
    "print (api)\n",
    "# Initialize the OpenAI model with custom parameters\n",
    "openai_model = OpenAIModel(\n",
    "    model_id=\"gpt-3.5-turbo\",  # Replace with your OpenAI model ID\n",
    "    client_args={\n",
    "        \"api_key\": api  # Replace with your OpenAI API key\n",
    "    },\n",
    "    params={\n",
    "        \"temperature\": 0.7,  # Adjust temperature for response randomness\n",
    "        \"max_tokens\": 100,  # Adjust max tokens for response length\n",
    "    }\n",
    ")\n",
    "# Create an agent with the OpenAI model\n",
    "agent = Agent(model=openai_model)\n",
    "# response = agent(prompt) # Output will fail without a valid OpenAI API key\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2c298",
   "metadata": {},
   "source": [
    "# # Hero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bec195",
   "metadata": {},
   "source": [
    "MCP is an open protocol that standardizes how applications provide context to large language models (LLMs).\n",
    "MCP has 3 components, 1/ a MCP server (can be local or remote), 2/ MCP client, and 3/ Transport layer protocol, which enable 4/ Data layer\n",
    "\n",
    " \n",
    "The protocol currently defines two standard transport mechanisms for client-server communication:\n",
    "1. stdio, communication over standard in and standard out\n",
    "2. Streamable HTTP\n",
    "3. custom transports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP tool - my first MCP \n",
    "\n",
    "from mcp import StdioServerParameters, stdio_client\n",
    "from strands import Agent\n",
    "from strands.tools.mcp import MCPClient\n",
    "# from mcp.client.streamable_http import streamablehttp_client\n",
    "# from mcp.server import FastMCP # FastMCP is a server implementation of MCP protocol\n",
    "\n",
    "# Connect to an MCP server using stdio transport\n",
    "stdio_mcp_client = MCPClient(\n",
    "    lambda: stdio_client(\n",
    "        StdioServerParameters(\n",
    "            command=\"uvx\", args=[\"awslabs.aws-documentation-mcp-server@latest\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create an agent with MCP tools\n",
    "with stdio_mcp_client:\n",
    "    # Get the tools from the MCP server\n",
    "    tools = stdio_mcp_client.list_tools_sync()\n",
    "    # print(tools)\n",
    "    \n",
    "    # Create an agent with these tools\n",
    "    agent = Agent(\n",
    "        model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        tools=tools)\n",
    "\n",
    "    response = agent(\"What is Amazon Bedrock pricing model. Be concise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb76fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"mcp[cli]\" nest_asyncio\n",
    "import nest_asyncio, sys\n",
    "nest_asyncio.apply()  # Apply the patch to allow nested event loops\n",
    "print(\"Python ready. Make sure Node.js/npm are installed on THIS machine to run 'npx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed480b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, asyncio, json\n",
    "from mcp import ClientSession\n",
    "from mcp.client.stdio import stdio_client\n",
    "# from mcp.client.session import get_initialized_session\n",
    "from mcp.types import ListToolsRequest\n",
    "from mcp.client.stdio import StdioServerParameters\n",
    "\n",
    "# --- REQUIRED: Twilio credentials ---\n",
    "# Recommended: create a Twilio API Key/Secret under your Account SID\n",
    "TWILIO_ACCOUNT_SID = \"AC12877baa7a81f8...c7\"   # e.g., AC... from Twilio Console\n",
    "TWILIO_API_KEY     = \"insert\"   # e.g., SK... from Twilio Console\n",
    "TWILIO_API_SECRET  = \"insert\"\n",
    "\n",
    "# Twilio MCP expects \"ACCOUNT_SID/API_KEY:API_SECRET\"\n",
    "CRED_ARG = f\"{TWILIO_ACCOUNT_SID}/{TWILIO_API_KEY}:{TWILIO_API_SECRET}\"\n",
    "\n",
    "# --- Optional: set a default region/data center via env if your account needs it\n",
    "env = os.environ.copy()\n",
    "# env[\"TWILIO_REGION\"] = \"us1\"  # example; only if you use regional routing\n",
    "\n",
    "# --- Configure the stdio server: this spawns the Twilio MCP server via npx\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",\n",
    "    args=[\"-y\", \"@twilio-alpha/mcp\", CRED_ARG],\n",
    "    env=env,\n",
    ")\n",
    "\n",
    "async def start_and_list_tools():\n",
    "    # stdio_client starts the child process and wires up stdin/stdout\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize per MCP spec (negotiates capabilities)\n",
    "            await session.initialize()\n",
    "\n",
    "            # List tools exposed by the Twilio MCP server\n",
    "            tools_resp = await session.list_tools()\n",
    "            tool_names = [t.name for t in tools_resp.tools]\n",
    "            print(f\"Discovered {len(tool_names)} tools:\")\n",
    "            for i, name in enumerate(tool_names, 1):\n",
    "                print(f\"{i:2d}. {name}\")\n",
    "            return tool_names, session\n",
    "\n",
    "# Run the coroutine and keep the session live by stashing it in globals\n",
    "tools, _ = asyncio.get_event_loop().run_until_complete(start_and_list_tools())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll re-open a session and keep it as a global for convenience.\n",
    "global _MCP_SESSION_CTX\n",
    "async def _open_session():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            yield session\n",
    "class SessionManager:\n",
    "    def __init__(self):\n",
    "        self._ctx = None\n",
    "        self.session = None\n",
    "    async def __aenter__(self):\n",
    "        self._ctx = _open_session()\n",
    "        self.session = await self._ctx.__anext__()  # enter the async generator\n",
    "        return self.session\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        try:\n",
    "            await self._ctx.__anext__()  # will raise StopAsyncIteration to close\n",
    "        except StopAsyncIteration:\n",
    "            pass\n",
    "MCP = SessionManager()\n",
    "print(\"SessionManager ready. Use `async with MCP as session:` to interact.\")\n",
    "\n",
    "import pprint, asyncio\n",
    "async def describe_tools():\n",
    "    async with MCP as session:\n",
    "        tools_resp = await session.list_tools()\n",
    "        for t in tools_resp.tools:\n",
    "            print(\"=\"*80)\n",
    "            print(\"Tool:\", t.name)\n",
    "            if t.inputSchema:\n",
    "                print(\"Input schema (JSON Schema):\")\n",
    "                pprint.pprint(t.inputSchema)\n",
    "            else:\n",
    "                print(\"No input schema advertised.\")\n",
    "asyncio.get_event_loop().run_until_complete(describe_tools())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
